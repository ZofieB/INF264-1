{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eef5abb4fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read file to input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_banknote_authentication.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#read file to input\n",
    "df = pd.read_csv('data_banknote_authentication.txt', sep=',', header = None)\n",
    "X = df.to_numpy()\n",
    "y = df[4].to_numpy()\n",
    "X = X[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node: \n",
    "    #instance attributes\n",
    "    def __init__(self, data, parent = None, majority_label = None):\n",
    "        # data contains a feature for inner nodes and the label for every leaf\n",
    "        self.data = data\n",
    "        self.parent = parent\n",
    "        self.majority_label = majority_label\n",
    "        #if children is empty, the node is a leaf -> data contains a label\n",
    "        self.children = [] #map value -> node // list of tuples\n",
    "    def __str__(self):\n",
    "        return f\"{self.data} - {self.children}\"\n",
    "    def addChild(self, child):\n",
    "        children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X, y, impurity_measure = 'entropy'):\n",
    "    if same_value(y):\n",
    "        #create a leaf with the label -> all labels are the same so we pick an arbitrary label\n",
    "        return node(y[0])\n",
    "    elif same_value(X):\n",
    "        #leaf with most common label\n",
    "        label = common_label(y)\n",
    "        return node(label)\n",
    "    else:\n",
    "        #select the desired feature based on impurity measure\n",
    "        feature = max_information_gain(X, y, impurity_measure)\n",
    "        #create feature node and split the dataset\n",
    "        f_node = node(feature, majority_label = )\n",
    "        X_right, X_left, y_right, y_left, val_avg = split_dataset(X, y, feature)\n",
    "        #create child nodes and set their parents\n",
    "        left_child = learn(delete_feature_from_matrix(X_left, feature), y_left)\n",
    "        left_child.parent = f_node\n",
    "        right_child = learn(delete_feature_from_matrix(X_right, feature), y_right)\n",
    "        right_child.parent = f_node\n",
    "        #add children to featurenode\n",
    "        f_node.addChild([left_child, val_avg])\n",
    "        f_node.addChild([right_child, val_avg])\n",
    "        return f_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hilfsfunktionen\n",
    "def same_value(list_in):\n",
    "    flat_array = np.ravel(list_in)\n",
    "    return np.all(list_in==flat_array[0])\n",
    "def common_label(y):\n",
    "    dict = {} \n",
    "    count = 0\n",
    "    item_ret = ''\n",
    "    for item in y: \n",
    "        dict[item] = dict.get(item, 0) + 1\n",
    "        if dict[item] >= count : \n",
    "            count, item_ret = dict[item], item \n",
    "    return(item_ret) \n",
    "def sort_with_index(liste):\n",
    "    temp_list = [ (liste[i],i) for i in range(len(liste)) ]\n",
    "    temp_list.sort()\n",
    "    return zip(*temp_list)\n",
    "\n",
    "def split_indices(index, y):\n",
    "    splitpoints = []\n",
    "    temp = index[0]\n",
    "    for i in range(len(index)):\n",
    "        if y[index[i]] != y[temp]:\n",
    "            #add current pair\n",
    "            splitpoints.append([temp, index[i]])\n",
    "            #start with next featurepoint\n",
    "            temp = index[i+1]\n",
    "    return splitpoints\n",
    "def max_information_gain(X, y, impurity_measure = 'entropy'):\n",
    "    max_ig = 0\n",
    "    max_label = 0\n",
    "    for i in range(len(X[0])):\n",
    "        x = X[:, i]\n",
    "        information_gain = information_gain(x, y, impurity_measure)\n",
    "        if information_gain > max_ig:\n",
    "            max_ig = information_gain\n",
    "            max_label = i\n",
    "    return max_label\n",
    "def split_dataset(X, y, feature):\n",
    "    feature_values = X[:, feature]\n",
    "    average = np.average(feature_values)\n",
    "    X_right = []\n",
    "    X_left = []\n",
    "    y_right = []\n",
    "    y_left = []\n",
    "    for i in range(len(feature_values)):\n",
    "        if feature_values[i] <= average:\n",
    "            X_left.append(X[i])\n",
    "            y_left.append(y[i])\n",
    "        else:\n",
    "            X_right.append(X[i])\n",
    "            y_right.append(X[i])\n",
    "    return X_right, X_left, y_right, y_left, average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-5c59b9897142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-f6ac7ac7d904>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#store average to next child node as the value with the child (value at the edge to the child node)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#call learn recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#decision value: average between the two feature values defining the point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def information_gain(x, y, impurity_measure=\"entropy\"):\n",
    "    #Calculate mean value of x\n",
    "    mean = 0\n",
    "    count = 0\n",
    "    for i in range(len(x)):\n",
    "        mean += x[i]\n",
    "        count +=1\n",
    "    mean /= count\n",
    "\n",
    "    #compute list of label_values for x<=mean or x>mean + probability of x <= mean\n",
    "    leq_mean = []\n",
    "    g_mean = []\n",
    "    count_leq_mean = 0\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= mean):\n",
    "            count_leq_mean +=1\n",
    "            leq_mean.append(y[i])\n",
    "        else:\n",
    "            g_mean.append(y[i])\n",
    "    prob_leq_mean = count_leq_mean / count\n",
    "\n",
    "    #compute H(y), H(y|x)\n",
    "    if(impurity_measure==\"entropy\"):\n",
    "        impurity = entropy(y)\n",
    "        conditional_impurity = prob_leq_mean * entropy(leq_mean)+ (1-prob_leq_mean) * entropy(g_mean)\n",
    "    elif(impurity_measure==\"gini\"):\n",
    "        impurity = gini(y)\n",
    "        conditional_impurity = prob_leq_mean * gini(leq_mean)+ (1-prob_leq_mean) * gini(g_mean) \n",
    "    else:\n",
    "        print(\"Not known impurity measure\")\n",
    "\n",
    "    #Information gain IG = H(y) - H(y|x)\n",
    "    return impurity - conditional_impurity\n",
    "\n",
    "def probabiliy_list(values):\n",
    "    count_values = len(values)\n",
    "    value_list = []\n",
    "    prob_list = []\n",
    "    different_val = 0\n",
    "    #compute list of the count of different values (unsorted)\n",
    "    for i in range(count_values):\n",
    "        if (values[i] not in value_list): \n",
    "            value_list.append(values[i])\n",
    "            prob_list.append(1)\n",
    "            different_val +=1\n",
    "        else:\n",
    "            for j in range(different_val):\n",
    "                if(values[i]==value_list[j]): prob_list[j]+=1\n",
    "    #get probabilities of different values by count[value] / count_all_values\n",
    "    for k in range(different_val):\n",
    "        prob_list[k] = float(prob_list[k]) / count_values\n",
    "    #Remark: this is an unsorted list of only the probabilities --> no information about the original values\n",
    "    return prob_list \n",
    "\n",
    "def entropy(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum -= math.log(prob_list[i], 2) * prob_list[i]\n",
    "    return sum\n",
    "\n",
    "def gini(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum += prob_list[i]*(1-prob_list[i])\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_column(x,index):\n",
    "    np.delete(x,index,0)\n",
    "\n",
    "#this function needs to be implemented in the tree class\n",
    "#class has variables: data, children\n",
    "#if leaf: data is label otherwise data is featureindex\n",
    "#children is a list with elements of form (value, node)\n",
    "#Nodes save sorted listed of values --> check < on everyone except last\n",
    "def predict(node, x):\n",
    "    count_values = len(node.children)\n",
    "    for i in range(count_values):\n",
    "        if (not node.children):                          #empty children list --> leave\n",
    "            return node.data\n",
    "        elif(i == count_values-1):                  #Last value in list --> x > elements in this tree\n",
    "            x_update = delete_column(x, node.data)       #not leafe --> data=feature_index\n",
    "            predict((node.children[i])[1], x_update)      #Remark: already used one feature!\n",
    "        elif(x[data] < (children[i])[0]):           #search child with right value\n",
    "            x_update = delete_column(x, node.data)\n",
    "            predict((node.children[i])[1],x_update)      #use predict on child recursivly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condition: Memorized majority label when building tree! --> majority_label\n",
    "#lets say X is 2-dimensional: X[Feature][number]\n",
    "def majority_label_acc(majority_label, y_prun):\n",
    "    x = 0\n",
    "    for i in range(len(y_prun)):\n",
    "        if(y_prun[i]==majority_label):\n",
    "            x += 1\n",
    "    return float(x) / len(y_prun)\n",
    "\n",
    "def acc(node, X, Y):\n",
    "    x = 0\n",
    "    for i in range(len(Y)):\n",
    "        if(Y[i]==predict(node, X[:,i])):\n",
    "            x += 1\n",
    "    return float(x) / len(Y)\n",
    "\n",
    "def delete_feature_from_matrix(X, Feature_index):\n",
    "    np.delete(X, Feature_index, 1)#Feature is Axis 1\n",
    "\n",
    "#this method has to be defined within the tree class\n",
    "def pruning(node, X_prun, y_prun): #starts with root + recursion\n",
    "    if (node.children): #the subtree is not a leaf\n",
    "        if (acc(node, X_prun, y_prun) < majority_label_acc(node.majority_label, y_prun)): #case: switch subtree with majority_label\n",
    "            node.data = node.majority_label\n",
    "            node.children = []\n",
    "        else: \n",
    "            for i in range(len(node.children)):\n",
    "                X_prun_update = delete_feature_from_matrix(X_train, node.data)#not leaf --> data = feature_index\n",
    "                pruning((node.children[i])[1], X_prun_update, y_prun) #call recursion on each child\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "X,Y = ...\n",
    "seed = 666\n",
    "X_train, X_val_test, Y_train, Y_val_test = model_selection.train_test_split(X, Y, test_size= 0.3, shuffle=True,                                                                                     random_state = seed)\n",
    "seed = 221\n",
    "X_val, X_test, Y_val, Y_test = model_selection.train_test_split(X, Y, test_size= 0.5, shuffle=True,                                                                                     random_state = seed)\n",
    "#We now have ratio training-, validation-, testdata: 0.7, 0.15, 0.15\n",
    "\n",
    "#Train decisiontree using entropy + pruning\n",
    "tree_ent_prun = learn(X_train, Y_train, impurity_measure='entropy', pruning=True)\n",
    "val_acc_ent_prun = acc(tree_ent_prun, X_val, Y_val)\n",
    "train_acc_ent_prun = acc(tree_ent_prun, X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using gini + pruning\n",
    "tree_gini_prun = learn(X_train, Y_train, impurity_measure='gini', pruning=True)\n",
    "val_acc_gini_prun = acc(tree_gini_prun, X_val, Y_val)\n",
    "train_acc_gini_prun = acc(tree_gini_prun, X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using entropy and not pruning\n",
    "tree_ent_notprun = learn(X_train, Y_train, impurity_measure='entropy', pruning=False)\n",
    "val_acc_ent_notprun = acc(tree_ent_notprun, X_val, Y_val)\n",
    "train_acc_ent_notprun = acc(tree_ent_notprun, X_train, Y_train)\n",
    "\n",
    "#Train decisiontree using gini and not pruning\n",
    "tree_gini_notprun = learn(X_train, Y_train, impurity_measure='gini', pruning=False)\n",
    "val_acc_gini_notprun = acc(tree_gini_notprun, X_val, Y_val)\n",
    "train_acc_gini_notprun = acc(tree_gini_notprun, X_train, Y_train)\n",
    "\n",
    "#plot accuracies\n",
    "settings_list = [ent_prun, gini_prun, ent_notprun, gini_notprun]\n",
    "train_accuracies = [train_acc_ent_prun, train_acc_gini_prun, train_acc_ent_notprun, train_acc_gini_notprun]\n",
    "val_accuracies = [val_acc_ent_prun, val_acc_gini_prun, val_acc_ent_notprun, val_acc_gini_notprun]\n",
    "plt.plot(settings_list, train_accuracies)\n",
    "plt.plot(settings_list, val_accuracies)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.title(\"Decision-tree accuracies with different settings\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ]
}