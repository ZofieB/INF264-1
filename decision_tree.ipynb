{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read file to input\n",
    "df = pd.read_csv('data_banknote_authentication.txt', sep=',', header = None)\n",
    "X = df.to_numpy()\n",
    "y = df[4].to_numpy()\n",
    "X = X[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node: \n",
    "    #instance attributes\n",
    "    def __init__(self, data, parent = None, majority_label = None):\n",
    "        # data contains a feature for inner nodes and the label for every leaf\n",
    "        self.data = data\n",
    "        self.parent = parent\n",
    "        self.majority_label = majority_label\n",
    "        #if children is empty, the node is a leaf -> data contains a label\n",
    "        self.children = [] #map value -> node // list of tuples\n",
    "    def __str__(self):\n",
    "        return f\"{self.data} - {self.children}\"\n",
    "    def addChild(self, child):\n",
    "        self.children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_rec(X, y, impurity_measure = 'entropy'):\n",
    "    if same_value(y):\n",
    "        #create a leaf with the label -> all labels are the same so we pick an arbitrary label\n",
    "        return node(y[0])\n",
    "    elif len(X[0]) == 0 or same_value(X):\n",
    "        #leaf with most common label\n",
    "        label = get_maj_label(y)\n",
    "        return node(label)\n",
    "    else:\n",
    "        #select the desired feature based on impurity measure\n",
    "        feature = max_information_gain(X, y, impurity_measure)\n",
    "        #create feature node and split the dataset\n",
    "        f_node = node(feature, majority_label = get_maj_label(y))\n",
    "        X_right, X_left, y_right, y_left, val_avg = split_dataset(X, y, feature)\n",
    "        #create child nodes and set their parents\n",
    "        left_child = learn_rec(delete_feature_from_matrix(X_left, feature), y_left)\n",
    "     #   print(\"Selected feature: \", feature, \"  new Matrix: \", delete_feature_from_matrix(X_left, feature))\n",
    "        left_child.parent = f_node\n",
    "        right_child = learn_rec(delete_feature_from_matrix(X_right, feature), y_right)\n",
    "        right_child.parent = f_node\n",
    "        #add children to featurenode\n",
    "        f_node.addChild([left_child, val_avg])\n",
    "        f_node.addChild([right_child, val_avg])\n",
    "        return f_node\n",
    "def learn(X, y, impurity_measure = 'entropy', pruning = True):\n",
    "    seed = 432\n",
    "#check pruning/training rate\n",
    "    X_train, X_prun, Y_train, Y_prun = model_selection.train_test_split(X, y, test_size= 0.3, shuffle=True, random_state = seed)\n",
    "    root = learn_rec(X_train, Y_train, impurity_measure)\n",
    "    if pruning:\n",
    "        root = pruning(root, X_prun, Y_prun)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hilfsfunktionen\n",
    "def same_value(list_in):\n",
    "    flat_array = np.ravel(list_in)\n",
    "    return np.all(list_in==flat_array[0])\n",
    "def common_label(y):\n",
    "    dict = {} \n",
    "    count = 0\n",
    "    item_ret = ''\n",
    "    for item in y: \n",
    "        dict[item] = dict.get(item, 0) + 1\n",
    "        if dict[item] >= count : \n",
    "            count, item_ret = dict[item], item \n",
    "    return(item_ret) \n",
    "def sort_with_index(liste):\n",
    "    temp_list = [ (liste[i],i) for i in range(len(liste)) ]\n",
    "    temp_list.sort()\n",
    "    return zip(*temp_list)\n",
    "\n",
    "def split_indices(index, y):\n",
    "    splitpoints = []\n",
    "    temp = index[0]\n",
    "    for i in range(len(index)):\n",
    "        if y[index[i]] != y[temp]:\n",
    "            #add current pair\n",
    "            splitpoints.append([temp, index[i]])\n",
    "            #start with next featurepoint\n",
    "            temp = index[i+1]\n",
    "    return splitpoints\n",
    "def max_information_gain(X, y, impurity_measure = 'entropy'):\n",
    "    max_ig = 0\n",
    "    max_label = 0\n",
    "    for i in range(len(X[0])):\n",
    "        x = X[:, i]\n",
    "        ig = information_gain(x, y, impurity_measure)\n",
    "        if ig > max_ig:\n",
    "            max_ig = ig\n",
    "            max_label = i\n",
    "    return max_label\n",
    "def split_dataset(X, y, feature):\n",
    "    feature_values = X[:, feature]\n",
    "    average = np.average(feature_values)\n",
    "    X_right = []\n",
    "    X_left = []\n",
    "    y_right = []\n",
    "    y_left = []\n",
    "    for i in range(len(feature_values)):\n",
    "        if feature_values[i] <= average:\n",
    "            X_left.append(X[i])\n",
    "            y_left.append(y[i])\n",
    "        else:\n",
    "            X_right.append(X[i])\n",
    "            y_right.append(X[i])\n",
    "    return X_right, X_left, y_right, y_left, average\n",
    "def get_maj_label(y):\n",
    "    label_list = []\n",
    "    count_list = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] not in label_list): \n",
    "            label_list.append(y[i])\n",
    "            count_list.append(1)\n",
    "        else:\n",
    "            for j in range(len(label_list)):\n",
    "                if(y[i]==label_list[j]): count_list[j]+=1\n",
    "    return label_list[np.argmax(count_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def information_gain(x, y, impurity_measure=\"entropy\"):\n",
    "    #Calculate mean value of x\n",
    "    mean = 0\n",
    "    count = 0\n",
    "    for i in range(len(x)):\n",
    "        mean += x[i]\n",
    "        count +=1\n",
    "    mean /= count\n",
    "\n",
    "    #compute list of label_values for x<=mean or x>mean + probability of x <= mean\n",
    "    leq_mean = []\n",
    "    g_mean = []\n",
    "    count_leq_mean = 0\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= mean):\n",
    "            count_leq_mean +=1\n",
    "            leq_mean.append(y[i])\n",
    "        else:\n",
    "            g_mean.append(y[i])\n",
    "    prob_leq_mean = count_leq_mean / count\n",
    "\n",
    "    #compute H(y), H(y|x)\n",
    "    if(impurity_measure==\"entropy\"):\n",
    "        impurity = entropy(y)\n",
    "        conditional_impurity = prob_leq_mean * entropy(leq_mean)+ (1-prob_leq_mean) * entropy(g_mean)\n",
    "    elif(impurity_measure==\"gini\"):\n",
    "        impurity = gini(y)\n",
    "        conditional_impurity = prob_leq_mean * gini(leq_mean)+ (1-prob_leq_mean) * gini(g_mean) \n",
    "    else:\n",
    "        print(\"Not known impurity measure\")\n",
    "\n",
    "    #Information gain IG = H(y) - H(y|x)\n",
    "    return impurity - conditional_impurity\n",
    "\n",
    "def probabiliy_list(values):\n",
    "    count_values = len(values)\n",
    "    value_list = []\n",
    "    prob_list = []\n",
    "    different_val = 0\n",
    "    #compute list of the count of different values (unsorted)\n",
    "    for i in range(count_values):\n",
    "        if (values[i] not in value_list): \n",
    "            value_list.append(values[i])\n",
    "            prob_list.append(1)\n",
    "            different_val +=1\n",
    "        else:\n",
    "            for j in range(different_val):\n",
    "                if(values[i]==value_list[j]): prob_list[j]+=1\n",
    "    #get probabilities of different values by count[value] / count_all_values\n",
    "    for k in range(different_val):\n",
    "        prob_list[k] = float(prob_list[k]) / count_values\n",
    "    #Remark: this is an unsorted list of only the probabilities --> no information about the original values\n",
    "    return prob_list \n",
    "\n",
    "def entropy(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum -= math.log(prob_list[i], 2) * prob_list[i]\n",
    "    return sum\n",
    "\n",
    "def gini(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum += prob_list[i]*(1-prob_list[i])\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_column(x,index):\n",
    "    return np.delete(x,index,0)\n",
    "\n",
    "#this function needs to be implemented in the tree class\n",
    "#class has variables: data, children\n",
    "#if leaf: data is label otherwise data is featureindex\n",
    "#children is a list with elements of form (value, node)\n",
    "#Nodes save sorted listed of values --> check < on everyone except last\n",
    "def predict(node, x):\n",
    "    count_values = len(node.children)\n",
    "    for i in range(count_values):\n",
    "        if (not node.children):                          #empty children list --> leave\n",
    "            return node.data\n",
    "        elif(i == count_values-1):                  #Last value in list --> x > elements in this tree\n",
    "            x_update = delete_column(x, node.data)       #not leafe --> data=feature_index\n",
    "            predict((node.children[i])[1], x_update)      #Remark: already used one feature!\n",
    "        elif(x[data] < (children[i])[0]):           #search child with right value\n",
    "            x_update = delete_column(x, node.data)\n",
    "            predict((node.children[i])[1],x_update)      #use predict on child recursivly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condition: Memorized majority label when building tree! --> majority_label\n",
    "#lets say X is 2-dimensional: X[Feature][number]\n",
    "def majority_label_acc(majority_label, y_prun):\n",
    "    x = 0\n",
    "    for i in range(len(y_prun)):\n",
    "        if(y_prun[i]==majority_label):\n",
    "            x += 1\n",
    "    return float(x) / len(y_prun)\n",
    "\n",
    "def acc(node, X, Y):\n",
    "    x = 0\n",
    "    for i in range(len(Y)):\n",
    "        if(Y[i]==predict(node, X[:,i])):\n",
    "            x += 1\n",
    "    return float(x) / len(Y)\n",
    "\n",
    "def delete_feature_from_matrix(X, Feature_index):\n",
    "    return np.delete(X, Feature_index, 1)#Feature is Axis 1\n",
    "\n",
    "#this method has to be defined within the tree class\n",
    "def pruning(node, X_prun, y_prun): #starts with root + recursion\n",
    "    if (node.children): #the subtree is not a leaf\n",
    "        if (acc(node, X_prun, y_prun) < majority_label_acc(node.majority_label, y_prun)): #case: switch subtree with majority_label\n",
    "            node.data = node.majority_label\n",
    "            node.children = []\n",
    "        else: \n",
    "            for i in range(len(node.children)):\n",
    "                X_prun_update = delete_feature_from_matrix(X_train, node.data)#not leaf --> data = feature_index\n",
    "                pruning((node.children[i])[1], X_prun_update, y_prun) #call recursion on each child\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3eed0b048217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Train decisiontree using entropy + pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtree_ent_prun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mval_acc_ent_prun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_ent_prun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_acc_ent_prun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_ent_prun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-55c7e60e0284>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(X, y, impurity_measure, pruning)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#check pruning/training rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_prun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_prun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpruning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_prun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_prun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-55c7e60e0284>\u001b[0m in \u001b[0;36mlearn_rec\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#create child nodes and set their parents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete_feature_from_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m      \u001b[0;31m#   print(\"Selected feature: \", feature, \"  new Matrix: \", delete_feature_from_matrix(X_left, feature))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mleft_child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-55c7e60e0284>\u001b[0m in \u001b[0;36mlearn_rec\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#create child nodes and set their parents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete_feature_from_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m      \u001b[0;31m#   print(\"Selected feature: \", feature, \"  new Matrix: \", delete_feature_from_matrix(X_left, feature))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mleft_child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-55c7e60e0284>\u001b[0m in \u001b[0;36mlearn_rec\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m     17\u001b[0m      \u001b[0;31m#   print(\"Selected feature: \", feature, \"  new Matrix: \", delete_feature_from_matrix(X_left, feature))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mleft_child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete_feature_from_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mright_child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#add children to featurenode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-55c7e60e0284>\u001b[0m in \u001b[0;36mlearn_rec\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#select the desired feature based on impurity measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#create feature node and split the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mf_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajority_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_maj_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-461405eeb021>\u001b[0m in \u001b[0;36mmax_information_gain\u001b[0;34m(X, y, impurity_measure)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mig\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_ig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mmax_ig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-42224ae93e95>\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(x, y, impurity_measure)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#compute H(y), H(y|x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpurity_measure\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimpurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mconditional_impurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_leq_mean\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleq_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprob_leq_mean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpurity_measure\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-42224ae93e95>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprob_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabiliy_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdifferent_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-42224ae93e95>\u001b[0m in \u001b[0;36mprobabiliy_list\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#compute list of the count of different values (unsorted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mvalue_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprob_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 666\n",
    "X_train, X_val_test, Y_train, Y_val_test = model_selection.train_test_split(X, y, test_size= 0.3, shuffle=True,                                                                                     random_state = seed)\n",
    "seed = 221\n",
    "X_val, X_test, Y_val, Y_test = model_selection.train_test_split(X_val_test, Y_val_test, test_size= 0.5, shuffle=True,                                                                                     random_state = seed)\n",
    "#We now have ratio training-, validation-, testdata: 0.7, 0.15, 0.15\n",
    "\n",
    "#Train decisiontree using entropy + pruning\n",
    "tree_ent_prun = learn(X_train, Y_train, impurity_measure='entropy', pruning=True)\n",
    "val_acc_ent_prun = acc(tree_ent_prun, X_val, Y_val)\n",
    "train_acc_ent_prun = acc(tree_ent_prun, X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using gini + pruning\n",
    "tree_gini_prun = learn(X_train, Y_train, impurity_measure='gini', pruning=True)\n",
    "val_acc_gini_prun = acc(tree_gini_prun, X_val, Y_val)\n",
    "train_acc_gini_prun = acc(tree_gini_prun, X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using entropy and not pruning\n",
    "tree_ent_notprun = learn(X_train, Y_train, impurity_measure='entropy', pruning=False)\n",
    "val_acc_ent_notprun = acc(tree_ent_notprun, X_val, Y_val)\n",
    "train_acc_ent_notprun = acc(tree_ent_notprun, X_train, Y_train)\n",
    "\n",
    "#Train decisiontree using gini and not pruning\n",
    "tree_gini_notprun = learn(X_train, Y_train, impurity_measure='gini', pruning=False)\n",
    "val_acc_gini_notprun = acc(tree_gini_notprun, X_val, Y_val)\n",
    "train_acc_gini_notprun = acc(tree_gini_notprun, X_train, Y_train)\n",
    "\n",
    "#plot accuracies\n",
    "settings_list = [ent_prun, gini_prun, ent_notprun, gini_notprun]\n",
    "train_accuracies = [train_acc_ent_prun, train_acc_gini_prun, train_acc_ent_notprun, train_acc_gini_notprun]\n",
    "val_accuracies = [val_acc_ent_prun, val_acc_gini_prun, val_acc_ent_notprun, val_acc_gini_notprun]\n",
    "plt.plot(settings_list, train_accuracies)\n",
    "plt.plot(settings_list, val_accuracies)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.title(\"Decision-tree accuracies with different settings\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}