{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def information_gain(x, y, impurity_measure=\"entropy\"):\n",
    "    mean = 0\n",
    "    count = 0\n",
    "    for i in range(len(x)):\n",
    "        mean += x[i]\n",
    "        count +=1\n",
    "    mean /= count\n",
    "    leq_mean = []\n",
    "    g_mean = []\n",
    "    count_leq_mean = 0\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= mean):\n",
    "            count_leq_mean +=1\n",
    "            leq_mean.append(y[i])\n",
    "        else:\n",
    "            g_mean.append(y[i])\n",
    "    prob_leq_mean = count_leq_mean / count\n",
    "    if(impurity_measure==\"entropy\"):\n",
    "        impurity = entropy(y)\n",
    "        conditional_impurity = prob_leq_mean * entropy(leq_mean)+ (1-prob_leq_mean) * entropy(g_mean)\n",
    "    elif(impurity_measure==\"gini\"):\n",
    "        impurity = gini(y)\n",
    "        conditional_impurity = prob_leq_mean * gini(leq_mean)+ (1-prob_leq_mean) * gini(g_mean) \n",
    "    else:\n",
    "        print(\"Not known impurity measure\")\n",
    "    return impurity - conditional_impurity\n",
    "\n",
    "def probabiliy_list(values):\n",
    "    count_values = len(values)\n",
    "    value_list = []\n",
    "    prob_list = []\n",
    "    different_val = 0\n",
    "    for i in range(count_values):\n",
    "        if (values[i] not in value_list): \n",
    "            value_list.append(values[i])\n",
    "            prob_list.append(1)\n",
    "            different_val +=1\n",
    "        else:\n",
    "            for j in range(different_val):\n",
    "                if(values[i]==value_list[j]): prob_list[j]+=1\n",
    "    for k in range(different_val):\n",
    "        prob_list[k] = float(prob_list[k]) / count_values\n",
    "    return prob_list \n",
    "\n",
    "def entropy(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum -= math.log(prob_list[i], 2) * prob_list[i]\n",
    "    return sum\n",
    "\n",
    "def gini(values):\n",
    "    prob_list = probabiliy_list(values)\n",
    "    different_values = len(prob_list)\n",
    "    sum = 0\n",
    "    for i in range(different_values):\n",
    "        sum += prob_list[i]*(1-prob_list[i])\n",
    "    return sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0\n0.5\n0.8112781244591328\n0.375\n"
    }
   ],
   "source": [
    "list = [1,1,2,2]\n",
    "print(entropy(list))\n",
    "print(gini(list))\n",
    "list2 = [1,1,1,2]\n",
    "print(entropy(list2))\n",
    "print(gini(list2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def delete_column(x,index):\n",
    "    np.delete(x,index,0)\n",
    "\n",
    "\n",
    "#class has variables: data, children\n",
    "#if leaf: data is label otherwise data is featureindex\n",
    "#children is a list with elements of form (value, node)\n",
    "#Nodes save sorted listed of values --> check < on everyone except last\n",
    "def predict(x):\n",
    "    count_values = len(children)\n",
    "    for i in range(count_values):\n",
    "        if (not children):                          #empty children list --> leave\n",
    "            return data\n",
    "        elif(i == count_values-1):                  #Last value in list --> x > elements in this tree\n",
    "            x_update = delete_column(x, data)       #not leafe --> data=feature_index\n",
    "            (children[i])[1].predict(x_update)      #Remark: already used one feature!\n",
    "        elif(x[data] < (children[i])[0]):           #search child with right value\n",
    "            x_update = delete_column(x, data)\n",
    "            (children[i])[1].predict(x_update)      #use predict on child recursivly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condition: Memorized majority label when building tree! --> majority_label\n",
    "\n",
    "def majority_label_acc(majority_label, y_prun):\n",
    "    x = 0\n",
    "    for i in range(len(y_prun)):\n",
    "        if(y_prun[i]==majority_label):\n",
    "            x += 1\n",
    "    return float(x) / len(y_prun)\n",
    "\n",
    "def acc(X, Y):\n",
    "    x = 0\n",
    "    for i in range(len(Y)):\n",
    "        if(Y[i]==predict(X[i])):#predict(X[i]) == i-th data-point in X\n",
    "            x += 1\n",
    "    return float(x) / len(Y)\n",
    "\n",
    "def delete_feature_from_matrix(X, Feature_index):\n",
    "    np.delete(X, Feature_index, 1)#Feature is Axis 1\n",
    "    \n",
    "def get_majority_label(Y):\n",
    "    count_values = len(Y)\n",
    "    label_list = []\n",
    "    count_list = []\n",
    "    different_val = 0\n",
    "    for i in range(count_values):\n",
    "        if (Y[i] not in label_list): \n",
    "            label_list.append(Y[i])\n",
    "            count_list.append(1)\n",
    "            different_val +=1\n",
    "        else:\n",
    "            for j in range(different_val):\n",
    "                if(Y[i]==label_list[j]): count_list[j]+=1\n",
    "    \n",
    "\n",
    "#this method has to be defined within the tree class\n",
    "def pruning(X_prun, y_prun): #starts with root + recursion\n",
    "    if (children): #the subtree is not a leaf\n",
    "        if (acc(X_prun, y_prun) < majority_label_acc(y_prun)): #case: switch subtree with majority_label\n",
    "            data = majority_label\n",
    "            children = []\n",
    "        else: \n",
    "            for i in range(len(children)):\n",
    "                X_prun_update = delete_feature_from_matrix(X_train, data)#not leaf --> data = feature_index\n",
    "                (children[i])[1].pruning(X_prun_update, y_prun) #call recursion on each child\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "X,Y = ...\n",
    "seed = 666\n",
    "X_train, X_val_test, Y_train, Y_val_test = model_selection.train_test_split(X, Y, test_size= 0.3, shuffle=True,                                                                                     random_state = seed)\n",
    "seed = 221\n",
    "X_val, X_test, Y_val, Y_test = model_selection.train_test_split(X, Y, test_size= 0.5, shuffle=True,                                                                                     random_state = seed)\n",
    "#We now have ratio training-, validation-, testdata: 0.7, 0.15, 0.15\n",
    "\n",
    "#Train decisiontree using entropy + pruning\n",
    "tree_ent_prun = tree.initialise() #no idea how implemented\n",
    "tree_ent_prun.learn(X_train, Y_train, impurity_measure='entropy', pruning=True)\n",
    "val_acc_ent_prun = tree_ent_prun.acc(X_val, Y_val)\n",
    "train_acc_ent_prun = tree_ent_prun.acc(X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using gini + pruning\n",
    "tree_gini_prun = tree.initialise()\n",
    "tree_gini_prun.learn(X_train, Y_train, impurity_measure='gini', pruning=True)\n",
    "val_acc_gini_prun = tree_gini_prun.acc(X_val, Y_val)\n",
    "train_acc_gini_prun = tree_gini_prun.acc(X_train, Y_val)\n",
    "\n",
    "#Train decisiontree using entropy and not pruning\n",
    "tree_ent_notprun = tree.initialise()\n",
    "tree_ent_notprun.learn(X_train, Y_train, impurity_measure='entropy', pruning=False)\n",
    "val_acc_ent_notprun = tree_ent_notprun.acc(X_val, Y_val)\n",
    "train_acc_ent_notprun = tree_ent_notprun.acc(X_train, Y_train)\n",
    "\n",
    "#Train decisiontree using gini and not pruning\n",
    "tree_gini_notprun = tree.initialise()\n",
    "tree_gini_notprun.learn(X_train, Y_train, impurity_measure='gini', pruning=False)\n",
    "val_acc_gini_notprun = tree_gini_notprun.acc(X_val, Y_val)\n",
    "train_acc_gini_notprun = tree_gini_notprun.acc(X_train, Y_train)\n",
    "\n",
    "#plot accuracies\n",
    "settings_list = [ent_prun, gini_prun, ent_notprun, gini_notprun]\n",
    "train_accuracies = [train_acc_ent_prun, train_acc_gini_prun, train_acc_ent_notprun, train_acc_gini_notprun]\n",
    "val_accuracies = [val_acc_ent_prun, val_acc_gini_prun, val_acc_ent_notprun, val_acc_gini_notprun]\n",
    "plt.plot(settings_list, train_accuracies)\n",
    "plt.plot(settings_list, val_accuracies)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.title(\"Decision-tree accuracies with different settings\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ]
}